# Tutorial-of-Data-Distillation-and-Condensation

## Main Idea
Data Distillation and Condensation (DDC) is a data-centric task where a representative (i.e., small but training-effective) batch of data is generated from the large dataset. Models trained on this small batch can obtain similar test performance compared to the models trained on the full dataset. Sometimes, the distilled images preserve certains aspects of semantics corresponding to the annotated objects in the full dataset, which are explainable to human users. A brief demostration of task is shown below:

<p align="center">
<img src="https://github.com/peterljq/Awesome-Data-Distillation-and-Condensation/blob/main/image/distillation_demo.png" width="500"/>
</p>
<p align="center"><em>DDC Basic Pipeline</em></p>

Some recommended works in this domain include:

- (2018) Dataset Distillation [[ArXiv](https://arxiv.org/abs/1811.10959)] [[Code](https://github.com/SsnL/dataset-distillation)] [[Project](https://ssnl.github.io/dataset_distillation/)]
- (CVPR 2022) Dataset Distillation by Matching Training Trajectories [[ArXiv](https://arxiv.org/abs/2203.11932)] [[Code](https://github.com/GeorgeCazenavette/mtt-distillation)] [[Project](https://georgecazenavette.github.io/mtt-distillation/)] [[Workshop Version](https://openaccess.thecvf.com/content/CVPR2022W/CVFAD/html/Cazenavette_Wearable_ImageNet_Synthesizing_Tileable_Textures_via_Dataset_Distillation_CVPRW_2022_paper.html)]


## Vision Approaches

## NLP Approaches

## Privacy
(ICML 2022 Oral) Privacy for Free: How does Dataset Condensation Help Privacy [[ArXiv](https://arxiv.org/abs/2206.00240)] [[Poster](https://icml.cc/virtual/2022/poster/18235)]

## Research Groups
